---
permalink: /
title: "About"
author_profile: true
redirect_from:
  - /about/
  - /about.html
---

I'm **Ruoqi Xing (邢若琦)**, an undergraduate student in Mathematics & Applied Mathematics at [Xidian University](https://www.xidian.edu.cn/), Xi'an, China.

Email: [24049200145@stu.xidian.edu.cn](mailto:24049200145@stu.xidian.edu.cn)

I’m interested in **learning-augmented optimization** and **evaluation pipelines**, and I’m currently exploring EOH (Evolution of Heuristics), neural optimizers (POM/EPOM-style), MoE routing, and reproducible black-box benchmarking.

## Research Interests

<div class="interest-pills">
  <span class="interest-pill">Learning-augmented Optimization</span>
  <span class="interest-pill">Neural Optimizers (POM/EPOM)</span>
  <span class="interest-pill">Mixture-of-Experts (MoE)</span>
  <span class="interest-pill">Black-box Optimization</span>
  <span class="interest-pill">Reproducible Benchmarking</span>
  <span class="interest-pill">Quantitative Finance</span>
</div>

## News

<div class="news-item">
  <span class="news-date">2025-12</span> · Exploring a MoE-based neural optimizer (MoE-POM) and improving evaluation protocols for cross-task robustness.
</div>
<div class="news-item">
  <span class="news-date">2025-12</span> · Experimenting with an LLM-assisted workflow for paper reading, algorithm design, and experiment automation.
</div>

## Experience

<div class="experience-item">
  <strong>Undergraduate student</strong> @ Xidian University<br>
  <small>2024 - Present</small><br>
  Interested in learning-augmented optimization: EOH pipelines, neural optimizer policy design, MoE routing, and reproducible benchmarking.
</div>

<div class="experience-item">
  <strong>B.S. in Mathematics & Applied Mathematics</strong> (信息数学拔尖班)<br>
  <small>Xidian University · 2024 - Present</small><br>
  Core training in analysis, algebra, probability, and optimization, with applied focus on ML systems.
</div>

## Projects

<div class="project-card">
  <h3>EOH (Evolution of Heuristics) Research Toolkit</h3>
  <p>A research codebase for generating, evaluating, and iterating on optimization heuristics with reproducible experiment runners and structured logging.</p>
  <span class="tag">Optimization</span>
  <span class="tag">Benchmarking</span>
  <span class="tag">Python</span>
</div>

<div class="project-card">
  <h3>MoE-POM (Mixture-of-Experts Neural Optimizer)</h3>
  <p>A MoE-routed optimizer policy inspired by POM/EPOM. Targets robustness across tasks and scale variation, with careful training stability and ablations.</p>
  <span class="tag">MoE</span>
  <span class="tag">Neural Optimizer</span>
  <span class="tag">Black-box Optimization</span>
</div>

<div class="project-card">
  <h3>LLM4AD Workflow</h3>
  <p>A Claude/LLM-assisted workflow for paper reading, algorithm design, and experiment automation. Focus on turning reading into runnable baselines.</p>
  <span class="tag">LLM</span>
  <span class="tag">Automation</span>
  <span class="tag">Research Tooling</span>
</div>

