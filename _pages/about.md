---
permalink: /
title: "About"
author_profile: true
redirect_from:
  - /about/
  - /about.html
---

I'm **Roky Xing (邢若琦)**, an undergraduate researcher in Mathematics & Applied Mathematics at [Xidian University](https://www.xidian.edu.cn/), Xi'an, China.

I build **learning-augmented optimizers** and **evaluation pipelines**, focusing on EOH (Evolution of Heuristics), neural optimizers (POM/EPOM-style), MoE routing, and reproducible black-box benchmarking.

## Research Interests

- Learning-augmented optimization (EOH, neural optimizers)
- POM/EPOM-style optimizer policies and training stability
- Mixture-of-Experts (MoE) for adaptive optimization
- Black-box optimization benchmarks (BBOB) and reproducible evaluation
- Efficient models and NAS
- Quantitative finance and algorithmic trading research

---

## News

- **2025-12**: Developing a MoE-based neural optimizer (MoE-POM) and tightening evaluation protocols for cross-task robustness.
- **2025-12**: Building an LLM4AD workflow for paper reading, algorithm design, and experiment automation.

---

## Experience

### Research Experience
- **2024 - Present**: Undergraduate Researcher @ Xidian University *(TODO: verify lab/advisor)*
  - Research on learning-augmented optimization: EOH pipelines, neural optimizer policy design (POM/EPOM-style), MoE routing, and reproducible benchmarking and ablation-driven iteration.

### Education
- **2024 - Present**: B.S. in Mathematics & Applied Mathematics (信息数学拔尖班) @ Xidian University, Xi'an, China
  - Core training in analysis, algebra, probability, and optimization, with applied focus on ML systems and evaluation.

---

## Projects

### EOH (Evolution of Heuristics) Research Toolkit
A research codebase for generating, evaluating, and iterating on optimization heuristics with reproducible experiment runners and structured logging.

**Tags**: Optimization, Benchmarking, Reproducibility, Python

---

### MoE-POM (Mixture-of-Experts Neural Optimizer)
A MoE-routed optimizer policy inspired by POM/EPOM. Targets robustness across tasks and scale variation, with careful training stability and ablations.

**Tags**: MoE, Neural Optimizer, Black-box Optimization

---

### LLM4AD Workflow
A Claude/LLM-assisted workflow for paper reading, algorithm design, and experiment automation. Focus on turning reading into runnable baselines and evaluations.

**Tags**: LLM, Automation, Research Tooling

---

## Awards

- **2025**: Mathematical Modeling Competition Award *(TODO: verify exact award name and organizer)*
- **2025**: Academic Ranking - Rank 2/31 in major, 4/31 comprehensive *(TODO: verify)*

---

## Talks

- **2025-12** *(TODO: verify)*: "From Memory to Thinking: Pretrained Optimizers with Adaptive Reasoning" - Internal seminar / competition presentation

---

*Last updated: December 2024*
